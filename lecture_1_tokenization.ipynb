{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture_1_tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pelinbalci/NLP_TF/blob/main/lecture_1_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVa45EGIXO0H"
      },
      "source": [
        "Ref: https://www.youtube.com/watch?v=f5YJA5mQD5c&ab_channel=GoogleDevelopers\n",
        "\n",
        "Common coding format is called ASCII.\n",
        "\n",
        "Common letters and symbols are encoded into values from 0 - 255\n",
        "\n",
        "It is useful in that 1 byte is needed to store the value for a letter. It has to be superseded by later encodings i.o to give access to characters and letters beyond 255. (in partivular international chars)\n",
        "\n",
        "\n",
        "LISTEN : 076, 073, 083 084, 069, 078  -> 6 bytes\n",
        "\n",
        "Here we will learn word-based encoding not letter-based.\n",
        "\n",
        "LISTEN and SILENT have same letters and opposite meanings :)\n",
        "\n",
        "A computer doesn't tell the difference btw these two words with letterbased ancoding unless we have a sequence model.( it is a bit complicated)\n",
        "\n",
        "\n",
        "**Word-Based Encoding**\n",
        "\n",
        "Each word respresented by a single number. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaCMcjMQifQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331ef859-7abf-44a5-a2b1-165319b7ff2d"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!'\n",
        "]\n",
        "\n",
        "# num words precifies the maximum number of words that you want to care about. \n",
        "# assign tokens to words based on how commonly used they are in corpus. \n",
        "# most common word will be at index 1. \n",
        "# ! is automtically removed:)\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100) \n",
        "tokenizer.fit_on_texts(sentences) \n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YEKJ6DJjz6L",
        "outputId": "df19b8b8-585a-4398-9740-79ce97db6695"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'merhaba, benim adım pelin',\n",
        "    'benim kedim var',\n",
        "    'sen neredesin?',\n",
        "    'sen buraya gel.',\n",
        "    'benim evim nerede nerede nerede',\n",
        "    'kedim',\n",
        "    'adım xxx',\n",
        "    'Sen',\n",
        "    'sen',\n",
        "    'Sen',\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "\n",
        "counts = tokenizer.word_counts\n",
        "print(counts)\n",
        "\n",
        "texttoseq = tokenizer.texts_to_sequences(sentences)\n",
        "print(texttoseq)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sen': 1, 'benim': 2, 'nerede': 3, 'adım': 4, 'kedim': 5, 'merhaba': 6, 'pelin': 7, 'var': 8, 'neredesin': 9, 'buraya': 10, 'gel': 11, 'evim': 12, 'xxx': 13}\n",
            "OrderedDict([('merhaba', 1), ('benim', 3), ('adım', 2), ('pelin', 1), ('kedim', 2), ('var', 1), ('sen', 5), ('neredesin', 1), ('buraya', 1), ('gel', 1), ('evim', 1), ('nerede', 3), ('xxx', 1)])\n",
            "[[6, 2, 4, 7], [2, 5, 8], [1, 9], [1, 10, 11], [2, 12, 3, 3, 3], [5], [4, 13], [1], [1], [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1r0G2_0yz4D",
        "outputId": "a56f4a11-bcaf-4854-fae3-93ca93254398"
      },
      "source": [
        "test_data = [\n",
        "             'adım nerede yazıyor?',\n",
        "             'benim kedim nerede',\n",
        "             'sen evde misin'\n",
        "]\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(test_seq)\n",
        "\n",
        "# In the frist sentence yazıyor is missing. \n",
        "# In the second sentence evde misin are missing. Since they are not in index. "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 3], [2, 5, 3], [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDjcDPDRzrD4"
      },
      "source": [
        "# How to deal with unseen word?\n",
        "\n",
        "Define oov: out of vocab in Tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6d2mj6X1VdB",
        "outputId": "1647cbd8-90ff-4fc3-d42b-bf34a2580bee"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'merhaba, benim adım pelin',\n",
        "    'benim kedim var',\n",
        "    'sen neredesin?',\n",
        "    'sen buraya gel.',\n",
        "    'benim evim nerede nerede nerede',\n",
        "    'kedim',\n",
        "    'adım xxx',\n",
        "    'Sen',\n",
        "    'sen',\n",
        "    'Sen',\n",
        "]\n",
        "\n",
        "test_data = [\n",
        "             'adım nerede yazıyor?',\n",
        "             'benim kedim nerede',\n",
        "             'sen evde misin'\n",
        "]\n",
        "\n",
        "# create tokenizer\n",
        "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
        "\n",
        "# fit data\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# get word index\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "\n",
        "# get index of words in each sentences\n",
        "sequence = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequence)\n",
        "\n",
        "# get index of words in test_data\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(test_seq)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'sen': 2, 'benim': 3, 'nerede': 4, 'adım': 5, 'kedim': 6, 'merhaba': 7, 'pelin': 8, 'var': 9, 'neredesin': 10, 'buraya': 11, 'gel': 12, 'evim': 13, 'xxx': 14}\n",
            "[[7, 3, 5, 8], [3, 6, 9], [2, 10], [2, 11, 12], [3, 13, 4, 4, 4], [6], [5, 14], [2], [2], [2]]\n",
            "[[5, 4, 1], [3, 6, 4], [2, 1, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63HxDkZ5HEKl"
      },
      "source": [
        "# Padding\n",
        "\n",
        "Before training a set we needed to have some level of uniformity of size. \n",
        "\n",
        "Each row is in the same length. \n",
        "\n",
        "If you want padding is located in post you need to add padding='post'. If you want you can limit the maxmum length with maxlen=5. If you have a sentence including more than 5 words, then you will loose info from previous word. You can also change it with truncating='post' "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGldNGfS2ISa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c84d4d10-8ed9-483f-c80a-21a8774dff1d"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = [\n",
        "    'merhaba, benim adım pelin',\n",
        "    'benim kedim var',\n",
        "    'sen neredesin?',\n",
        "    'sen buraya gel.',\n",
        "    'benim evim nerede nerede nerede biliyor',\n",
        "    'kedim',\n",
        "    'adım xxx',\n",
        "    'Sen',\n",
        "    'sen',\n",
        "    'Sen',\n",
        "]\n",
        "\n",
        "# create tokenizer\n",
        "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
        "\n",
        "# fit data\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# get word index\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "\n",
        "# get index of words in each sentences\n",
        "sequence = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequence)\n",
        "\n",
        "# padding\n",
        "print('basic padded')\n",
        "padded = pad_sequences(sequence)\n",
        "print(padded)\n",
        "\n",
        "print('padded post')\n",
        "padded_post = pad_sequences(sequence, padding='post')\n",
        "print(padded_post)\n",
        "\n",
        "print('padded post, maxlen')\n",
        "padded_post_max = pad_sequences(sequence, padding='post', maxlen=4)\n",
        "print(padded_post_max)\n",
        "\n",
        "print('padded post, maxlen post')\n",
        "padded_post_max_post = pad_sequences(sequence, padding='post', truncating='post', maxlen=4)\n",
        "print(padded_post_max_post)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'sen': 2, 'benim': 3, 'nerede': 4, 'adım': 5, 'kedim': 6, 'merhaba': 7, 'pelin': 8, 'var': 9, 'neredesin': 10, 'buraya': 11, 'gel': 12, 'evim': 13, 'biliyor': 14, 'xxx': 15}\n",
            "[[7, 3, 5, 8], [3, 6, 9], [2, 10], [2, 11, 12], [3, 13, 4, 4, 4, 14], [6], [5, 15], [2], [2], [2]]\n",
            "basic padded\n",
            "[[ 0  0  7  3  5  8]\n",
            " [ 0  0  0  3  6  9]\n",
            " [ 0  0  0  0  2 10]\n",
            " [ 0  0  0  2 11 12]\n",
            " [ 3 13  4  4  4 14]\n",
            " [ 0  0  0  0  0  6]\n",
            " [ 0  0  0  0  5 15]\n",
            " [ 0  0  0  0  0  2]\n",
            " [ 0  0  0  0  0  2]\n",
            " [ 0  0  0  0  0  2]]\n",
            "padded post\n",
            "[[ 7  3  5  8  0  0]\n",
            " [ 3  6  9  0  0  0]\n",
            " [ 2 10  0  0  0  0]\n",
            " [ 2 11 12  0  0  0]\n",
            " [ 3 13  4  4  4 14]\n",
            " [ 6  0  0  0  0  0]\n",
            " [ 5 15  0  0  0  0]\n",
            " [ 2  0  0  0  0  0]\n",
            " [ 2  0  0  0  0  0]\n",
            " [ 2  0  0  0  0  0]]\n",
            "padded post, maxlen\n",
            "[[ 7  3  5  8]\n",
            " [ 3  6  9  0]\n",
            " [ 2 10  0  0]\n",
            " [ 2 11 12  0]\n",
            " [ 4  4  4 14]\n",
            " [ 6  0  0  0]\n",
            " [ 5 15  0  0]\n",
            " [ 2  0  0  0]\n",
            " [ 2  0  0  0]\n",
            " [ 2  0  0  0]]\n",
            "padded post, maxlen post\n",
            "[[ 7  3  5  8]\n",
            " [ 3  6  9  0]\n",
            " [ 2 10  0  0]\n",
            " [ 2 11 12  0]\n",
            " [ 3 13  4  4]\n",
            " [ 6  0  0  0]\n",
            " [ 5 15  0  0]\n",
            " [ 2  0  0  0]\n",
            " [ 2  0  0  0]\n",
            " [ 2  0  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrQQxg1wJRsS",
        "outputId": "3bafb737-d54b-456c-faef-5c92cc5ffeeb"
      },
      "source": [
        "test_data = [\n",
        "             'adım nerede yazıyor?',\n",
        "             'benim kedim nerede',\n",
        "             'sen evde misin acaba benimle gelecek'\n",
        "]\n",
        "test_sequence = tokenizer.texts_to_sequences(test_data)\n",
        "\n",
        "# padding\n",
        "print('basic padded')\n",
        "padded = pad_sequences(test_sequence)\n",
        "print(padded)\n",
        "\n",
        "print('padded post')\n",
        "padded_post = pad_sequences(test_sequence, padding='post')\n",
        "print(padded_post)\n",
        "\n",
        "print('padded post, maxlen')\n",
        "padded_post_max = pad_sequences(test_sequence, padding='post', maxlen=4)\n",
        "print(padded_post_max)\n",
        "\n",
        "print('padded post, maxlen post')\n",
        "padded_post_max_post = pad_sequences(test_sequence, padding='post', truncating='post', maxlen=4)\n",
        "print(padded_post_max_post)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "basic padded\n",
            "[[0 0 0 5 4 1]\n",
            " [0 0 0 3 6 4]\n",
            " [2 1 1 1 1 1]]\n",
            "padded post\n",
            "[[5 4 1 0 0 0]\n",
            " [3 6 4 0 0 0]\n",
            " [2 1 1 1 1 1]]\n",
            "padded post, maxlen\n",
            "[[5 4 1 0]\n",
            " [3 6 4 0]\n",
            " [1 1 1 1]]\n",
            "padded post, maxlen post\n",
            "[[5 4 1 0]\n",
            " [3 6 4 0]\n",
            " [2 1 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}